{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Ngram language model lab SOLUTIONS\n",
    "\n",
    "In this lab you will do 4 exercises building and evaluating ngram language models of the following types:\n",
    "1. A Maximum Likelihood Estimation (MLE) unigram model\n",
    "2. A bigram model with add-one Laplace smoothing\n",
    "3. A bigram model with general additive/Lidstone (add-k) smoothing\n",
    "4. Ngram models with an advanced interpolation technique, Kneser-Ney snoothing (the methods are provided)\n",
    "\n",
    "Before you start the exercises, make sure you run and understand the examples first. Then complete the exercises using the following 3 files with line-separated text to train the bigger language models on:\n",
    "* training data -- \"switchboard_lm_training.txt\"\n",
    "* heldout/development data -- \"switchboard_lm_heldout.txt\"\n",
    "* test data -- \"switchboard_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "LOG_BASE = 2 # all the way through here we will use log base 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful methods for all exercises:\n",
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of sentences (corpus) from the example in the lecture slides\n",
    "sentences = [\n",
    "            \"I am Sam\",\n",
    "            \"Sam I am\",\n",
    "            \"I do not like green eggs and ham\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Build a unigram MLE language model from a simple corpus.\n",
    "An MLE unigram model will tell you how likely a word is to occur, estimated from the function of counts:\n",
    "\n",
    "C(w_i)/N\n",
    "\n",
    "where C(w_i) is the number of times the word at position i occurred in the training corpus, and N is the sum of the counts of all words, or, to put it another way, the length of the training corpus.\n",
    "\n",
    "Notice the tokenization method adds a `</s>` at the end but no `<s>` is needed at the beginning of each sentence\n",
    "    because unigrams do not have a context word (we are only concerned with the frequency of single words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['I', 'am', 'Sam', '</s>']\n",
      "tokenized ['Sam', 'I', 'am', '</s>']\n",
      "tokenized ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n",
      "11 different unigrams observed\n",
      "unigram total 17\n"
     ]
    }
   ],
   "source": [
    "unigrams = Counter()\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    print(\"tokenized\", words)\n",
    "    for w in words:\n",
    "        unigrams[w] +=1\n",
    "unigram_total = sum(unigrams.values()) # to get the denominator for unigram probabilities\n",
    "print(len(unigrams), \"different unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will measure the model's perplexity of those same training sentences. Note in normal practice you would want to do this on different data (as you will do below).\n",
    "\n",
    "Perplexity is equal to 2 to the power of the cross entropy where cross entropy is the negative sum of all log probabilities from the model normalized by the length of the corpus N.\n",
    "\n",
    "Measure the cross entropy and perplexity on each sentence too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Sam', '</s>'] cross entropy: 2.7949815908897615 perplexity: 6.940220937885672\n",
      "['Sam', 'I', 'am', '</s>'] cross entropy: 2.7949815908897615 perplexity: 6.940220937885672\n",
      "['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>'] cross entropy: 3.7352489522011934 perplexity: 13.317477627933627\n",
      "unigram corpus cross entropy 3.2927701939369913\n",
      "unigram corpus perplexity 9.799921507045037\n"
     ]
    }
   ],
   "source": [
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in sentences:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    words = tokenize_sentence(sent, 1) # tokenize sentence with the order 1 as the parameter\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in words:\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, LOG_BASE)  # the log of the prob to base 2\n",
    "        s += -log(prob, LOG_BASE) # add the neg log prob to s\n",
    "        sent_s += -log(prob, LOG_BASE)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N  # cross entropy total neg log probs/length for this sentence\n",
    "    sent_perplexity = LOG_BASE ** sent_cross_entropy # perplexity for the sentence 2 to the cross-entropy\n",
    "    print(words, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N # cross entropy of corpus total neg log probs/length\n",
    "perplexity = 2 ** cross_entropy  # perplexity for the corpus 2 to the cross-entropy\n",
    "print(\"unigram corpus cross entropy\", cross_entropy)\n",
    "print(\"unigram corpus perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Build a bigram MLE language model from the same corpus\n",
    "A MLE (unsmoothed) bigram model will tell you how likely a word is to occur given the previous word, estimated from the function of counts:\n",
    "\n",
    "C(w_i-1, w_i)/C(w_i-1)\n",
    "\n",
    "where for any pairs of contiguous words w_i-1, w_i, C(w_i-1, w_i) is the number of times the word at position i follows the word at position i-1 in the training corpus, and C(w_i-1) is the number of times the word at position i-1 occurs in the corpus. E.g. for the bigram probability of 'john likes', C(w_i-1, w_i) is the number of times 'john likes' occurs, and C(w_i-1) is how many times 'john' occurs.\n",
    "\n",
    "Notice the tokenization method adds a `</s>` at the end and also one `<s>` for padding at the beginning as we want to count the number of times the word at the beginning of each sentence begins a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 different bigrams\n",
      "11 different bigram contexts (and unigrams) observed\n"
     ]
    }
   ],
   "source": [
    "# First get the counts from the training corpus for bigrams without smoothing\n",
    "bigrams = Counter() # a counter for how many times a given bigram sequence w_i-1,w_i occurs\n",
    "bigram_context = Counter() # a counter for how many times each word is used as a context word w_i-1 (so will include the start symbol)\n",
    "order = 2 # order (i.e. n) of the language model- bigram n=2\n",
    "for s in sentences:\n",
    "    words = tokenize_sentence(s, order)  # tokenize sentence with the order 2 as the parameter\n",
    "    for i in range(order - 1, len(words)):\n",
    "        context = words[i-order+1:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        bigrams[glue_tokens(ngram, order)] +=1\n",
    "        bigram_context[glue_tokens(context, 1)] += 1\n",
    "print(len(bigrams.keys()), \"different bigrams\")\n",
    "print(len(bigram_context.keys()), \"different bigram contexts (and unigrams) observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a handy function to calculate the probability of an bigram from the counts\n",
    "def prob_bigram_MLE(ngram):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = numerator / denominator\n",
    "    return prob\n",
    "\n",
    "\n",
    "if False: # optional- check to see if continuation distributions sum to 1\n",
    "    # check if each bigram continuation distribution sums to one\n",
    "    # look at the distributions of possible continuations after each word\n",
    "    for context, v in bigram_context.items():\n",
    "        context = unglue_tokens(context, 1)\n",
    "        print(\"%% context\", context)\n",
    "        check_ngram_total_sums_to_1 = 0\n",
    "        # for a given context the continuation probabilities \n",
    "        # over the whole vocab should sum to 1\n",
    "        for u in unigrams.keys():\n",
    "            ngram = context + [u]\n",
    "            prob = prob_bigram_MLE(ngram)\n",
    "            print(ngram, prob)\n",
    "            check_ngram_total_sums_to_1 += prob\n",
    "        print(\"sums to 1?\", check_ngram_total_sums_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.5\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Check the estimates for the lecture examples:\n",
    "# p(I|<s>)\n",
    "# p(Sam|<s>)\n",
    "# p(am|I)\n",
    "# p(</s>|Sam)\n",
    "# p(Sam|am)\n",
    "# p(do|I)\n",
    "\n",
    "print(prob_bigram_MLE(['<s>','I']))\n",
    "print(prob_bigram_MLE(['<s>', 'Sam']))\n",
    "print(prob_bigram_MLE(['I', 'am']))\n",
    "print(prob_bigram_MLE(['Sam', '</s>']))\n",
    "print(prob_bigram_MLE(['am', 'Sam']))\n",
    "print(prob_bigram_MLE(['I', 'do']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, as in the unigram case above we will measure the model's perplexity of those same training sentences.\n",
    "\n",
    "Notice that even with this small corpus the bigram perplexity is significantly lower than the unigram perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'am', 'Sam', '</s>'] cross entropy: 0.7924812503605781 perplexity: 1.7320508075688774\n",
      "['<s>', 'Sam', 'I', 'am', '</s>'] cross entropy: 1.042481250360578 perplexity: 2.0597671439071177\n",
      "['<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>'] cross entropy: 0.24110277793803472 perplexity: 1.1818957424692271\n",
      "bigram corpus cross entropy 0.5593985296662904\n",
      "bigram corpus perplexity 1.4736547115524326\n"
     ]
    }
   ],
   "source": [
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, order)\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for i in range(order - 1, len(words)):\n",
    "        context = words[i-order+1:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_MLE(ngram)\n",
    "        s += -log(prob, LOG_BASE) # add the neg log prob to s\n",
    "        sent_s += -log(prob, LOG_BASE)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N  # cross entropy total neg log probs/length for this sentence\n",
    "    sent_perplexity = LOG_BASE ** sent_cross_entropy # perplexity for the sentence 2 to the cross-entropy\n",
    "    print(words, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N # cross entropy of corpus total neg log probs/length\n",
    "perplexity = 2 ** cross_entropy  # perplexity for the corpus 2 to the cross-entropy\n",
    "print(\"bigram corpus cross entropy\", cross_entropy)\n",
    "print(\"bigram corpus perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Defining Vocabulary, using OOV word token and building a unigram MLE model from a bigger corpus\n",
    "\n",
    "Write code to read in the file `switchboard_lm_train.txt` which has preprocessed text on each line.\n",
    "\n",
    "You will populate a unigram language model based on that data for an MLE estimation using a `Counter` using this data (see Example 1 above as to how this is done for a smaller dataset).\n",
    "\n",
    "Before you do this, you have to define a vocabulary of words using the training data, which you will keep the same for all of the following exercises. In these exercises, the vocabulary is defined by using a Minimum Document Frequency of 2 in the training data. That means the vocab should only contain words which occur at least twice in the training data.\n",
    "\n",
    "Any words not in the vocabulary in the training, heldout and testing data must be replaced with an out-of-vocab symbol `<unk/>` before processing them.\n",
    " \n",
    "Using this model, calculate the perplexity of the ENTIRE test corpus `switchboard_lm_test.txt`- again, remember to replace words unknown by the model with `<unk/>` before getting these measures. See Example 1 as to how this is done for a unigram model on the smaller example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12729\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# first get the vocab\n",
    "\n",
    "corpus = open(\"switchboard_lm_train.txt\")\n",
    "initial_unigrams = Counter()\n",
    "for line in corpus:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    for w in words:\n",
    "        initial_unigrams[w]+=1\n",
    "corpus.close()\n",
    "\n",
    "vocab = []\n",
    "for w,v in initial_unigrams.items():\n",
    "    if v>=2:\n",
    "        vocab.append(w)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown words in training data 7898\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# next do the replacement of out-of-vocab (OOV) words with <unk/> in training sentences\n",
    "\n",
    "train_sentences = []\n",
    "corpus = open(\"switchboard_lm_train.txt\")\n",
    "unknown_words = 0\n",
    "for line in corpus:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    final_words = []\n",
    "    for w in words[:-1]:\n",
    "        word = w\n",
    "        if w not in vocab:\n",
    "            word = \"<unk/>\"\n",
    "            unknown_words += 1\n",
    "        final_words.append(word)\n",
    "    train_sentences.append(\" \".join(final_words))\n",
    "print(\"unknown words in training data\", unknown_words)\n",
    "corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "for sent in train_sentences:\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    for word in words:\n",
    "        unigrams[word]+=1\n",
    "unigram_total = sum(unigrams.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306313\n"
     ]
    }
   ],
   "source": [
    "print(unigram_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown words in test data 628\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# next do the replacement of out-of-vocab (OOV) words with <unk/> in test sentences\n",
    "\n",
    "test_sentences = []\n",
    "corpus = open(\"switchboard_lm_test.txt\")\n",
    "unknown_words = 0\n",
    "for line in corpus:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    final_words = []\n",
    "    for w in words[:-1]:\n",
    "        word = w\n",
    "        if w not in vocab:\n",
    "            word = \"<unk/>\"\n",
    "            unknown_words +=1\n",
    "        final_words.append(word)\n",
    "    test_sentences.append(\" \".join(final_words))\n",
    "print(\"unknown words in test data\", unknown_words)\n",
    "corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Q1) cross entropy 8.287702715147693\n",
      "(Q1) perplexity 312.4979066155026\n"
     ]
    }
   ],
   "source": [
    "N = 0 # total number of words\n",
    "s = 0 # neg sum of log probs\n",
    "for sent in test_sentences:\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    for word in words:\n",
    "        N += 1\n",
    "        num = unigrams[word]   # numerator is count of unigram\n",
    "        denom = unigram_total  # denominator is overall length of training data\n",
    "        prob = num / denom\n",
    "        log_prob = log(prob, 2)\n",
    "        s += -log_prob\n",
    "\n",
    "perplexity = 2 ** (s/N)\n",
    "print(\"(Q1) cross entropy\", s/N)\n",
    "print(\"(Q1) perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 2. Bigram model with add-one smoothing\n",
    "\n",
    "Change your method for reading in and training a language model from Exercise 1 so it works for bigrams. Use the same methods for identifying and replacing out-of-vocabulary words as you did in Exercise 1 (i.e. use the same vocabulary).\n",
    "\n",
    "However, in testing, rather than just using the raw counts for implementing MLE probabilities you should implement add-one smoothing (see the lecture notes and Jurafsky & Martin Chapter 3/Manning and Schuetze Chapter 6). Remember this involves using the vocabulary size in the denominator of the formula.\n",
    "\n",
    "Obtain the perplexity score on the test data as above for this smoothed bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts for the bigram model\n",
    "bigrams = Counter()  # counts all the bigrams\n",
    "bigram_context = Counter() # counts unigrams, but the previous word only (so includes the start symbol, not the end)\n",
    "delta = 1  # delta is order - 1\n",
    "vocab = vocab + [\"<s>\"] # add the start symbol for bigrams\n",
    "\n",
    "for sent in train_sentences:\n",
    "    words = tokenize_sentence(sent, 2)\n",
    "    for i in range(delta, len(words)):\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        bigrams[glue_tokens(ngram, 2)] +=1\n",
    "        bigram_context[glue_tokens(context, 1)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add-1 smoothing\n",
      "cross entropy 8.248721413034907\n",
      "perplexity 304.1673339728869\n"
     ]
    }
   ],
   "source": [
    "# get the test results\n",
    "# get the entropy/perplexity for plus 1 smoothing on the TEST data (Q2)\n",
    "# for best result, test it on the test data\n",
    "N = 0 # total number of words\n",
    "s = 0 # mass of neglogprob (entropy)\n",
    "for sent in test_sentences:\n",
    "    words = tokenize_sentence(sent, 2)\n",
    "    for i in range(delta, len(words)):\n",
    "        N += 1\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        num = bigrams[glue_tokens(ngram, 2)] + 1 # add 1 to the numerator\n",
    "        denom = bigram_context[glue_tokens(context, 1)] + len(bigram_context.keys()) # add V to the denominator\n",
    "        prob = num/denom\n",
    "        log_prob = log(prob, 2)\n",
    "        s += (-log_prob)\n",
    "print(\"add-1 smoothing\")\n",
    "perplexity = 2 ** (s/N)\n",
    "print(\"cross entropy\", s/N)\n",
    "print(\"perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Bigram model with general additive (Lidstone) add-k smoothing\n",
    "\n",
    "Modify your code from Exercise 2 such that it generalizes beyond adding 1 to all counts, but can add differing amounts k of mass instead, to implement general additive add-k smoothing.\n",
    "\n",
    "On the HELDOUT corpus `switchboard_lm_heldout.txt` experiment with different values of k (e.g. 0.2, 0.4, 0.6, 0.8, though try others if you can) and report the perplexity scores for these different values in a comment. You could also use an optimization algorithm from scipy.optimize to search this efficiently.\n",
    "\n",
    "Once you find the value which gives you the lowest perplexity on the heldout data, use this model to get the perplexity of the test data once and report the scores in a comment. You should be able to get better (lower) perplexity scores than plus-1 smoothing, however make sure the vocabulary used is the same, else it is not a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown words in heldout set 634\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# next do the replacement of out-of-vocab (OOV) words with <unk/> in heldout sentences\n",
    "heldout_sentences = []\n",
    "unknown_words = 0\n",
    "corpus = open(\"switchboard_lm_heldout.txt\")\n",
    "for line in corpus:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    final_words = []\n",
    "    for w in words[:-1]:\n",
    "        word = w\n",
    "        if w not in vocab:\n",
    "            word = \"<unk/>\"\n",
    "            unknown_words += 1\n",
    "        final_words.append(word)\n",
    "    heldout_sentences.append(\" \".join(final_words))\n",
    "print(\"unknown words in heldout set\", unknown_words)\n",
    "corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add-0.001 6.914173989499943 120.60734746883352\n",
      "add-0.002 6.859854832769601 116.15076394061568\n",
      "add-0.003 6.838662047978613 114.45701267621813\n",
      "add-0.004 6.829189781774097 113.70798679465172\n",
      "add-0.005 6.825360477621457 113.40657513110997\n",
      "add-0.006 6.824686853265728 113.35363559997062\n",
      "add-0.007 6.825941774864039 113.45227863068037\n",
      "add-0.008 6.8284452414699635 113.64931996979587\n",
      "add-0.009 6.8317891784575275 113.91304648576887\n",
      "add-0.01 6.835714045927112 114.22337008930648\n",
      "add-0.1 7.199013500070242 146.93288389670292\n",
      "add-0.2 7.439684502618534 173.60738423639475\n",
      "add-0.4 7.753563156324377 215.80181019748358\n",
      "add-0.6 7.971728174404209 251.0321223111287\n",
      "add-0.8 8.141488581198528 282.37891962673933\n"
     ]
    }
   ],
   "source": [
    "# test a range of k for add-k smoothing\n",
    "\n",
    "results = {}\n",
    "for raw_weight in [i for i in range(1, 11)] + [100, 200, 400, 600, 800]:  # test 0.001..0.01 and 0.1, 0.2, 0.4, etc.\n",
    "    weight = raw_weight/1000\n",
    "\n",
    "    N = 0 # total number of words\n",
    "    s = 0 # mass of neglogprob (entropy)\n",
    "    for sent in heldout_sentences:\n",
    "        words = tokenize_sentence(sent, 2)\n",
    "        for i in range(delta, len(words)):\n",
    "            N += 1\n",
    "            context = words[i-delta:i]\n",
    "            target = words[i]\n",
    "            ngram = context + [target]\n",
    "            num = bigrams[glue_tokens(ngram, 2)] + weight  # add k to numerator\n",
    "            denom = bigram_context[glue_tokens(context, 1)] + (weight * len(bigram_context.keys()))\n",
    "            prob = num / denom\n",
    "            log_prob = log(prob, 2)\n",
    "            s += (-log_prob)\n",
    "\n",
    "    perplexity = 2 ** (s/N)\n",
    "    print(\"add-\" + str(weight), s/N, perplexity)\n",
    "    results[weight] = (s/N, perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Q3) weight 0.8 cross-entropy, perplexity: (8.141488581198528, 282.37891962673933)\n",
      "(Q3) weight 0.6 cross-entropy, perplexity: (7.971728174404209, 251.0321223111287)\n",
      "(Q3) weight 0.4 cross-entropy, perplexity: (7.753563156324377, 215.80181019748358)\n",
      "(Q3) weight 0.2 cross-entropy, perplexity: (7.439684502618534, 173.60738423639475)\n",
      "(Q3) weight 0.1 cross-entropy, perplexity: (7.199013500070242, 146.93288389670292)\n",
      "(Q3) weight 0.01 cross-entropy, perplexity: (6.835714045927112, 114.22337008930648)\n",
      "(Q3) lowest cross-entropy, perplexity model (0.006, (6.824686853265728, 113.35363559997062))\n"
     ]
    }
   ],
   "source": [
    "#print(\"(Q2) weight 1 (add-1 smoothing) cross-entropy, perplexity:\", results[1.0])\n",
    "print(\"(Q3) weight 0.8 cross-entropy, perplexity:\", results[0.8])\n",
    "print(\"(Q3) weight 0.6 cross-entropy, perplexity:\", results[0.6])\n",
    "print(\"(Q3) weight 0.4 cross-entropy, perplexity:\", results[0.4])\n",
    "print(\"(Q3) weight 0.2 cross-entropy, perplexity:\", results[0.2])\n",
    "print(\"(Q3) weight 0.1 cross-entropy, perplexity:\", results[0.1])\n",
    "print(\"(Q3) weight 0.01 cross-entropy, perplexity:\", results[0.01])\n",
    "print(\"(Q3) lowest cross-entropy, perplexity model\", min(results.items(), key=lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aroound 0.006 seems to be the best (lowest cross-entropy/peplexity)\n",
    "# more principled way of optimizing, even to 4 decimal places:\n",
    "def perplexity_of_add_k_bigram(weight, sentences):\n",
    "    N = 0 # total number of words\n",
    "    s = 0 # mass of neglogprob (entropy)\n",
    "    for sent in sentences:\n",
    "        words = tokenize_sentence(sent, 2)\n",
    "        for i in range(delta, len(words)):\n",
    "            N += 1\n",
    "            context = words[i-delta:i]\n",
    "            target = words[i]\n",
    "            ngram = context + [target]\n",
    "            num = bigrams[glue_tokens(ngram, 2)] + weight  # add k to numerator\n",
    "            denom = bigram_context[glue_tokens(context, 1)] + (weight * len(bigram_context.keys()))\n",
    "            prob = num / denom\n",
    "            log_prob = log(prob, 2)\n",
    "            s += (-log_prob)\n",
    "\n",
    "    perplexity = 2 ** (s/N)\n",
    "    print(\"add-\" + str(weight), s/N, perplexity)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add-[0.01] 6.835714045927112 114.22337008930648\n",
      "add-[0.0105] 6.837838042764665 114.39165843026665\n",
      "add-[0.0095] 6.833691336774233 114.0633371220369\n",
      "add-[0.009] 6.8317891784575275 113.91304648576887\n",
      "add-[0.008] 6.8284452414699635 113.64931996979587\n",
      "add-[0.007] 6.825941774864039 113.45227863068037\n",
      "add-[0.005] 6.825360477621457 113.40657513110997\n",
      "add-[0.003] 6.838662047978613 114.45701267621813\n",
      "add-[0.003] 6.838662047978613 114.45701267621813\n",
      "add-[0.006] 6.824686853265728 113.35363559997062\n",
      "add-[0.007] 6.825941774864039 113.45227863068037\n",
      "add-[0.0055] 6.824723732385953 113.35653325724958\n",
      "add-[0.0065] 6.825124246723737 113.38800714337503\n",
      "add-[0.00575] 6.824639034502369 113.3498785059953\n",
      "add-[0.0055] 6.824723732385953 113.35653325724958\n",
      "add-[0.005875] 6.8246473305047255 113.35053030941613\n",
      "add-[0.005625] 6.824663831647001 113.35182678848572\n",
      "add-[0.0058125] 6.824639165729114 113.34988881623819\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 113.349879\n",
      "         Iterations: 9\n",
      "         Function evaluations: 18\n",
      "best k 0.0057499999999999895\n"
     ]
    }
   ],
   "source": [
    "# use scipy optimize to get even better performance\n",
    "from scipy import optimize\n",
    "best = optimize.minimize(\n",
    "    perplexity_of_add_k_bigram,\n",
    "    0.01,    # first argument that to be optimized\n",
    "    args=(heldout_sentences),  # other arguments to the function\n",
    "    method='Nelder-Mead', # use nelder mead to find minima\n",
    "    tol=0.0001, # to this degree of error\n",
    "    options={'disp': True}\n",
    ")\n",
    "best_k = best.x[0]\n",
    "print(\"best k\", best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add-0.0057499999999999895 6.806850670324508 111.96086052571216\n",
      "best-k perplexity on test 111.96086052571216\n"
     ]
    }
   ],
   "source": [
    "# get test result\n",
    "print(\"best-k perplexity on test\", perplexity_of_add_k_bigram(best_k, test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 4. Ngram models with Kneser-Ney smoothing\n",
    "\n",
    "Kneser-Ney smoothing is a state-of-the-art technique for smoothing n-gram models.\n",
    "\n",
    "The algorithm is quite complicated, and is implemented for you below for training/getting the appropriate counts  on the training data (`count_ngrams_interpolated_kneser_ney()`). The training process is implemented below for a trigram model, but without doing the appropriate out-of-vocab word replacement as you've done above, using exactly the same vocabulary.\n",
    "\n",
    "The application at test time is done with the method `kneser_ney_ngram_prob()` using the trained Counters, which gives the probability of the model applied to an ngram of a given order, with a given discount.\n",
    "\n",
    "Try to follow how the training works and how the application of the model to ngrams works, and refer to both Section 3.5 in Jurafsky and Martin and the below article on QM plus (pages 7-8 particularly):\n",
    "\n",
    "\"A Bit of Progress in Language Modeling\" (2001) - Joshua T. Goodman\n",
    "\n",
    "In this exercise, you will first modify the training part of the code so it does the replacement of out-of-vocab words as you did in the previous exercises. You do not need to modify the methods below.\n",
    "\n",
    "On the HELDOUT corpus experiment with different orders from trigram upwards (try 3, 4 and 5) and different discount values (e.g. 0.2, 0.4, 0.6, 0.8, though try others if you can) and report the perplexity scores for these different values in a comment. You could also use an optimization algorithm from scipy.optimize to search the different n values and discount values efficiently. \n",
    "\n",
    "Once you find the order and discount values which gives you the lowest perplexity on the heldout data, use this model to get the perplexity of the TEST data once and report the scores in a comment. You should be able to beat (get a lower) perplexity for this compared to excercises 1-3, though make sure you are always using the same vocabulary, else it is not a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney smoothing\n",
    "order = 3  # use a trigram\n",
    "delta = order - 1\n",
    "discount = 0.8  # use a discount of 0.8\n",
    "\n",
    "\n",
    "# initialize the maps and counts\n",
    "unigram_denominator = 0\n",
    "ngram_numerator_map = Counter() \n",
    "ngram_denominator_map = Counter() \n",
    "ngram_non_zero_map = Counter()\n",
    "\n",
    "\n",
    "def count_ngrams_interpolated_kneser_ney(tokens,\n",
    "                                   order,\n",
    "                                   ngram_numerator_map,\n",
    "                                   ngram_denominator_map,\n",
    "                                   ngram_non_zero_map,\n",
    "                                   unigram_denominator):\n",
    "    \"\"\"Function used in n-gram language model training\n",
    "    to count the n-grams in tokens and also record the\n",
    "    lower order non -ero counts necessary for interpolated Kneser-Ney\n",
    "    smoothing.\n",
    "    \n",
    "    Taken from Goodman 2001 and generalized to arbitrary orders\"\"\"\n",
    "    for i in range(order-1,len(tokens)): # tokens should have a prefix of order - 1\n",
    "        #print(i)\n",
    "        for d in range(order,0,-1): #go through all the different 'n's backwards\n",
    "            if d == 1:\n",
    "                unigram_denominator += 1   # it has been in a context\n",
    "                ngram_numerator_map[glue_tokens(tokens[i],d)] += 1\n",
    "            else:\n",
    "                den_key = glue_tokens(tokens[i-(d-1) : i], d)\n",
    "                num_key = glue_tokens(tokens[i-(d-1) : i+1], d)\n",
    "                # increment the number of times the denominator/context has been seen by 1\n",
    "                ngram_denominator_map[den_key] += 1\n",
    "                # we store the numerator value to check if it's 0\n",
    "                tmp = ngram_numerator_map[num_key]\n",
    "                # we increment the number of times it's been observed as a numerator\n",
    "                ngram_numerator_map[num_key] += 1\n",
    "                # this incrementation of the n-gram count if d < order\n",
    "                # will only happen if the ngram tested at d+1 was unique\n",
    "                if tmp == 0:\n",
    "                    # if this is the first time we see this ngram\n",
    "                    # increment number of types for which its context\n",
    "                    # has been used as a context for any continuation\n",
    "                    ngram_non_zero_map[den_key] += 1\n",
    "                else:\n",
    "                    break \n",
    "                    # if the ngram has already been seen\n",
    "                    # we don't go down to any lower order models\n",
    "                    # because they will have already been counted as types\n",
    "    # return the updated counts and maps\n",
    "    return ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneser_ney_ngram_prob(ngram, discount, order):\n",
    "    \"\"\"KN smoothed ngram probability from Goodman 2001.\n",
    "    This is run at test time to calculate the probability\n",
    "    of a given n-gram or a given order with a given discount.\n",
    "    \n",
    "    ngram :: list of strings, the ngram\n",
    "    discount :: float, the discount used\n",
    "    order :: int, order of the model\n",
    "    \"\"\"\n",
    "    # First, calculate the unigram continuation prob of the last token\n",
    "    # If we've never seen it at all, it will \n",
    "    # have no probability as a numerator\n",
    "    uni_num = ngram_numerator_map.get(glue_tokens(ngram[-1], 1)) # number of bigrams the word is a continuation for\n",
    "    if not uni_num: # if no value found in dict, make it 0\n",
    "        uni_num = 0\n",
    "    # unigram_denominator is the number of different bigram types (not tokens)\n",
    "    probability = previous_prob = uni_num / unigram_denominator\n",
    "    \n",
    "    # Check: Given <unk/> should have been used in place of unknown words before passing\n",
    "    # to this method, probability should be non-zero\n",
    "    if probability == 0.0:\n",
    "        print(\"0 prob for unigram!\")\n",
    "        print(glue_tokens(ngram[-1], 1))\n",
    "        print(ngram)\n",
    "        print(ngram_numerator_map.get(glue_tokens(ngram[-1], 1)))\n",
    "        print(unigram_denominator)\n",
    "        raise Exception\n",
    "\n",
    "    # Compute the higher order probs (from 2/bi-gram upwards) and interpolate them\n",
    "    for d in range(2,order+1):\n",
    "        # Get the context count for the denominator:\n",
    "        # When d = order (n) this is the number of times it's observed in the corpus (tokens)\n",
    "        # When d < order (n) this is the number of different continuation types (not tokens) seen with it as its prefix\n",
    "        ngram_denom = ngram_denominator_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "        if not ngram_denom: # if no value found in dict, make it 0\n",
    "            ngram_denom = 0\n",
    "        if ngram_denom != 0:\n",
    "            # Get the ngram count for the numerator\n",
    "            # When d = order (n) this is the number of times it's observed in the corpus (tokens)\n",
    "            # When d < order (n) this is the number of types of unique ngram for n=d+1 it is a continuation for (types)\n",
    "            ngram_num = ngram_numerator_map.get(glue_tokens(ngram[-(d):], d))\n",
    "            if not ngram_num: # if no value found in dict, make it 0\n",
    "                ngram_num = 0\n",
    "            if ngram_num != 0:\n",
    "                # calculate the prob with fixed discount\n",
    "                current_prob = (ngram_num - discount) / ngram_denom\n",
    "            else:\n",
    "                current_prob = 0.0\n",
    "            # get the number of word types that can follow the context\n",
    "            # (number of times normalised discount has been applied):\n",
    "            nonzero = ngram_non_zero_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "            if not nonzero: # if no value found in dict, make it 0\n",
    "                nonzero = 0\n",
    "            # get the lambda for this context\n",
    "            lambda_context = discount / ngram_denom * nonzero\n",
    "            # interpolate with previous probability of lower orders calculated so far\n",
    "            current_prob += lambda_context * previous_prob\n",
    "            previous_prob = current_prob\n",
    "            probability = current_prob\n",
    "        else:\n",
    "            #if this context (e.g. bigram context for trigrams) has never been seen, \n",
    "            #then we can only use the last order with a probability (e.g. unigram)\n",
    "            #and halt\n",
    "            probability = previous_prob\n",
    "            break\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount 0.6\n",
      "3 0.6 6.241801452580393 75.67796830136515\n",
      "discount 0.61\n",
      "3 0.61 6.2357080364676465 75.35900630992558\n",
      "discount 0.62\n",
      "3 0.62 6.229811114113146 75.05160992150178\n",
      "discount 0.63\n",
      "3 0.63 6.224108202646403 74.75551973583148\n",
      "discount 0.64\n",
      "3 0.64 6.218597182550095 74.47050225716646\n",
      "discount 0.65\n",
      "3 0.65 6.213276298293761 74.19634930355728\n",
      "discount 0.66\n",
      "3 0.66 6.208144161801208 73.93287761070887\n",
      "discount 0.67\n",
      "3 0.67 6.203199758956959 73.67992863536205\n",
      "discount 0.68\n",
      "3 0.68 6.198442459413827 73.43736856662021\n",
      "discount 0.69\n",
      "3 0.69 6.1938720300411445 73.20508855809508\n",
      "discount 0.7\n",
      "3 0.7 6.189488652456602 72.983005199455\n",
      "discount 0.71\n",
      "3 0.71 6.185292945187324 72.77106125157445\n",
      "discount 0.72\n",
      "3 0.72 6.181285991158633 72.56922667760851\n",
      "discount 0.73\n",
      "3 0.73 6.177469371385676 72.37750001161491\n",
      "discount 0.74\n",
      "3 0.74 6.173845205956239 72.19591011748707\n",
      "discount 0.75\n",
      "3 0.75 6.170416203700359 72.02451840685906\n",
      "discount 0.76\n",
      "3 0.76 6.167185722296468 71.86342160294313\n",
      "discount 0.77\n",
      "3 0.77 6.164157841063159 71.71275516295331\n",
      "discount 0.78\n",
      "3 0.78 6.161337449316755 71.57269750422354\n",
      "discount 0.79\n",
      "3 0.79 6.1587303540403235 71.44347522358304\n",
      "discount 0.8\n",
      "3 0.8 6.156343411773763 71.32536955933497\n",
      "discount 0.81\n",
      "3 0.81 6.15418469122655 71.21872442698258\n",
      "discount 0.82\n",
      "3 0.82 6.152263675332017 71.12395647383603\n",
      "discount 0.83\n",
      "3 0.83 6.150591514631354 71.04156776060728\n",
      "discount 0.84\n",
      "3 0.84 6.149181348370976 70.97216190958855\n",
      "discount 0.85\n",
      "3 0.85 6.148048716383128 70.91646490341915\n",
      "discount 0.86\n",
      "3 0.86 6.147212094770381 70.87535223163498\n",
      "discount 0.87\n",
      "3 0.87 6.1466936037706015 70.84988487470656\n",
      "discount 0.88\n",
      "3 0.88 6.146519960360805 70.84135786456216\n",
      "discount 0.89\n",
      "3 0.89 6.146723787608154 70.85136720030286\n",
      "discount 0.9\n",
      "3 0.9 6.147345459258133 70.88190433840678\n",
      "discount 0.91\n",
      "3 0.91 6.148435775016153 70.93549353498702\n",
      "discount 0.92\n",
      "3 0.92 6.15005997784283 71.01539850770094\n",
      "discount 0.93\n",
      "3 0.93 6.152304046928119 71.12594679593367\n",
      "discount 0.94\n",
      "3 0.94 6.155285087290599 71.27306626416586\n",
      "discount 0.95\n",
      "3 0.95 6.159169674758123 71.46523406882805\n",
      "discount 0.96\n",
      "3 0.96 6.164209270523979 71.71531163813766\n",
      "discount 0.97\n",
      "3 0.97 6.170817697877571 72.0445652276295\n",
      "discount 0.98\n",
      "3 0.98 6.179776327795721 72.49332858451893\n",
      "discount 0.99\n",
      "3 0.99 6.193000786159116 73.16089333361225\n",
      "discount 1.0\n",
      "3 1.0 6.224983220132772 74.80087390005508\n",
      "discount 0.6\n",
      "4 0.6 6.32173318385284 79.98919256734635\n",
      "discount 0.61\n",
      "4 0.61 6.310883838155776 79.38991453845604\n",
      "discount 0.62\n",
      "4 0.62 6.300331167696529 78.81133135781494\n",
      "discount 0.63\n",
      "4 0.63 6.290070514835182 78.25280245350473\n",
      "discount 0.64\n",
      "4 0.64 6.280097756314168 77.71373826798248\n",
      "discount 0.65\n",
      "4 0.65 6.270409299686299 77.19359806974447\n",
      "discount 0.66\n",
      "4 0.66 6.261002083700958 76.6918881326145\n",
      "discount 0.67\n",
      "4 0.67 6.251873582862484 76.20816027472455\n",
      "discount 0.68\n",
      "4 0.68 6.243021816483492 75.742010756768\n",
      "discount 0.69\n",
      "4 0.69 6.234445362636025 75.29307954475561\n",
      "discount 0.7\n",
      "4 0.7 6.2261433775387 74.86104995080143\n",
      "discount 0.71\n",
      "4 0.71 6.21811562106844 74.44564867425858\n",
      "discount 0.72\n",
      "4 0.72 6.210362489263971 74.04664627552815\n",
      "discount 0.73\n",
      "4 0.73 6.2028850549182355 73.66385812729312\n",
      "discount 0.74\n",
      "4 0.74 6.1956851176482415 73.29714590328092\n",
      "discount 0.75\n",
      "4 0.75 6.188765265200006 72.94641968376409\n",
      "discount 0.76\n",
      "4 0.76 6.182128948217046 72.61164078108169\n",
      "discount 0.77\n",
      "4 0.77 6.175780571336963 72.29282542072276\n",
      "discount 0.78\n",
      "4 0.78 6.169725604279294 71.99004945379843\n",
      "discount 0.79\n",
      "4 0.79 6.163970717717259 71.70345433354063\n",
      "discount 0.8\n",
      "4 0.8 6.158523950183522 71.43325466164534\n",
      "discount 0.81\n",
      "4 0.81 6.153394914329644 71.17974771414453\n",
      "discount 0.82\n",
      "4 0.82 6.148595053677368 70.94332549797535\n",
      "discount 0.83\n",
      "4 0.83 6.14413796505474 70.72449009307438\n",
      "discount 0.84\n",
      "4 0.84 6.140039807684201 70.5238733251602\n",
      "discount 0.85\n",
      "4 0.85 6.136319828424412 70.34226224397594\n",
      "discount 0.86\n",
      "4 0.86 6.133001045436476 70.1806325253948\n",
      "discount 0.87\n",
      "4 0.87 6.130111152190458 70.04019290739232\n",
      "discount 0.88\n",
      "4 0.88 6.127683734747305 69.9224453378489\n",
      "discount 0.89\n",
      "4 0.89 6.125759945782385 69.82926807008792\n",
      "discount 0.9\n",
      "4 0.9 6.124390864109637 69.7630332658996\n",
      "discount 0.91\n",
      "4 0.91 6.1236409185234555 69.72677828367242\n",
      "discount 0.92\n",
      "4 0.92 6.123593031944027 69.72446391965357\n",
      "discount 0.93\n",
      "4 0.93 6.1243566846136135 69.76138050006547\n",
      "discount 0.94\n",
      "4 0.94 6.126081236659267 69.84482091010914\n",
      "discount 0.95\n",
      "4 0.95 6.128979476505956 69.9852736752864\n",
      "discount 0.96\n",
      "4 0.96 6.133373150360531 70.19873609286003\n",
      "discount 0.97\n",
      "4 0.97 6.13979281254962 70.51180039077775\n",
      "discount 0.98\n",
      "4 0.98 6.149243429169796 70.97521598767128\n",
      "discount 0.99\n",
      "4 0.99 6.164206252002217 71.71516158979327\n",
      "discount 1.0\n",
      "4 1.0 6.204572935490571 73.75009156760723\n",
      "discount 0.6\n",
      "5 0.6 6.397004795799044 84.2733631523835\n",
      "discount 0.61\n",
      "5 0.61 6.383162661402387 83.46865747463698\n",
      "discount 0.62\n",
      "5 0.62 6.369672415324392 82.69180266483126\n",
      "discount 0.63\n",
      "5 0.63 6.356528028335745 81.94181981521773\n",
      "discount 0.64\n",
      "5 0.64 6.343724099233174 81.21780314042937\n",
      "discount 0.65\n",
      "5 0.65 6.331255848223993 80.51891619428775\n",
      "discount 0.66\n",
      "5 0.66 6.319119114837498 79.84438861154909\n",
      "discount 0.67\n",
      "5 0.67 6.3073103605905265 79.19351335317596\n",
      "discount 0.68\n",
      "5 0.68 6.2958266767289714 78.5656444420083\n",
      "discount 0.69\n",
      "5 0.69 6.28466579747713 77.96019518428636\n",
      "discount 0.7\n",
      "5 0.7 6.273826119370451 77.37663688236034\n",
      "discount 0.71\n",
      "5 0.71 6.263306727409037 76.81449805425974\n",
      "discount 0.72\n",
      "5 0.72 6.253107428968093 76.27336418765674\n",
      "discount 0.73\n",
      "5 0.73 6.243228796655109 75.75287807007608\n",
      "discount 0.74\n",
      "5 0.74 6.233672221621257 75.25274075443927\n",
      "discount 0.75\n",
      "5 0.75 6.224439979241216 74.77271324046713\n",
      "discount 0.76\n",
      "5 0.76 6.215535309587813 74.31261897890951\n",
      "discount 0.77\n",
      "5 0.77 6.206962515824134 73.87234734106048\n",
      "discount 0.78\n",
      "5 0.78 6.19872708451502 73.45185824036398\n",
      "discount 0.79\n",
      "5 0.79 6.190835833089446 73.05118815467013\n",
      "discount 0.8\n",
      "5 0.8 6.1832970912843255 72.67045787764962\n",
      "discount 0.81\n",
      "5 0.81 6.1761209256533345 72.30988244045196\n",
      "discount 0.82\n",
      "5 0.82 6.169319419335512 71.9697838000277\n",
      "discount 0.83\n",
      "5 0.83 6.162907023679932 71.65060711040785\n",
      "discount 0.84\n",
      "5 0.84 6.156901004669185 71.35294171056923\n",
      "discount 0.85\n",
      "5 0.85 6.151322016417816 71.07754842980866\n",
      "discount 0.86\n",
      "5 0.86 6.146194847997669 70.8253955123436\n",
      "discount 0.87\n",
      "5 0.87 6.141549411357495 70.59770654350325\n",
      "discount 0.88\n",
      "5 0.88 6.1374220720839805 70.39602546951845\n",
      "discount 0.89\n",
      "5 0.89 6.1338574800855055 70.22230659196687\n",
      "discount 0.9\n",
      "5 0.9 6.13091115076203 70.07904213811567\n",
      "discount 0.91\n",
      "5 0.91 6.128653211657931 69.96944832598689\n",
      "discount 0.92\n",
      "5 0.92 6.127174033468972 69.89774624080367\n",
      "discount 0.93\n",
      "5 0.93 6.126593059451137 69.86960405009052\n",
      "discount 0.94\n",
      "5 0.94 6.127073399593519 69.89287075737302\n",
      "discount 0.95\n",
      "5 0.95 6.128847637987709 69.97887846843726\n",
      "discount 0.96\n",
      "5 0.96 6.132267756099228 70.14497035729791\n",
      "discount 0.97\n",
      "5 0.97 6.137914693174145 70.42006692467974\n",
      "discount 0.98\n",
      "5 0.98 6.146890068216451 70.85953378233292\n",
      "discount 0.99\n",
      "5 0.99 6.1619186844579295 71.60153862929413\n",
      "discount 1.0\n",
      "5 1.0 6.205213612170486 73.78285002000437\n"
     ]
    }
   ],
   "source": [
    "# search over n == 3, 4, 5\n",
    "# try discounts from 0.60 to 1.00 in 0.01 increments\n",
    "results  = {}\n",
    "for order in range(3,6):\n",
    "    # reset each time\n",
    "    delta = order -1\n",
    "    unigram_denominator = 0\n",
    "    ngram_numerator_map = Counter() \n",
    "    ngram_denominator_map = Counter() \n",
    "    ngram_non_zero_map = Counter()\n",
    "\n",
    "    # for each n- get the counts from training\n",
    "    for sent in train_sentences:\n",
    "        tokens = tokenize_sentence(sent, order)\n",
    "        # correct use of kneser-ney probability function:\n",
    "        ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "                count_ngrams_interpolated_kneser_ney(tokens, order,\n",
    "                                               ngram_numerator_map, ngram_denominator_map,\n",
    "                                      ngram_non_zero_map, unigram_denominator)\n",
    "\n",
    "    for raw_discount in range(60, 101):\n",
    "        discount = raw_discount / 100\n",
    "        #print(\"*\" * 30)\n",
    "        #print(\"order\", order)\n",
    "        print(\"discount\", discount)\n",
    "        \n",
    "        N = 0\n",
    "        s = 0\n",
    "        unknown_words = 0\n",
    "        for sent in heldout_sentences:\n",
    "            tokens = tokenize_sentence(sent, order)\n",
    "            for i in range(order-1, len(tokens)):\n",
    "                N +=1\n",
    "                ngram = tokens[i-delta:i+1]\n",
    "                kn_prob = kneser_ney_ngram_prob(ngram, discount, order)\n",
    "                s+= (-log(kn_prob, 2))\n",
    "        perplexity = 2 ** (s/N)\n",
    "        print(order, discount, s/N, perplexity)\n",
    "        results[(order, discount)] = (s/N, perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(3, 0.6): (6.241801452580393, 75.67796830136515),\n",
       " (3, 0.61): (6.2357080364676465, 75.35900630992558),\n",
       " (3, 0.62): (6.229811114113146, 75.05160992150178),\n",
       " (3, 0.63): (6.224108202646403, 74.75551973583148),\n",
       " (3, 0.64): (6.218597182550095, 74.47050225716646),\n",
       " (3, 0.65): (6.213276298293761, 74.19634930355728),\n",
       " (3, 0.66): (6.208144161801208, 73.93287761070887),\n",
       " (3, 0.67): (6.203199758956959, 73.67992863536205),\n",
       " (3, 0.68): (6.198442459413827, 73.43736856662021),\n",
       " (3, 0.69): (6.1938720300411445, 73.20508855809508),\n",
       " (3, 0.7): (6.189488652456602, 72.983005199455),\n",
       " (3, 0.71): (6.185292945187324, 72.77106125157445),\n",
       " (3, 0.72): (6.181285991158633, 72.56922667760851),\n",
       " (3, 0.73): (6.177469371385676, 72.37750001161491),\n",
       " (3, 0.74): (6.173845205956239, 72.19591011748707),\n",
       " (3, 0.75): (6.170416203700359, 72.02451840685906),\n",
       " (3, 0.76): (6.167185722296468, 71.86342160294313),\n",
       " (3, 0.77): (6.164157841063159, 71.71275516295331),\n",
       " (3, 0.78): (6.161337449316755, 71.57269750422354),\n",
       " (3, 0.79): (6.1587303540403235, 71.44347522358304),\n",
       " (3, 0.8): (6.156343411773763, 71.32536955933497),\n",
       " (3, 0.81): (6.15418469122655, 71.21872442698258),\n",
       " (3, 0.82): (6.152263675332017, 71.12395647383603),\n",
       " (3, 0.83): (6.150591514631354, 71.04156776060728),\n",
       " (3, 0.84): (6.149181348370976, 70.97216190958855),\n",
       " (3, 0.85): (6.148048716383128, 70.91646490341915),\n",
       " (3, 0.86): (6.147212094770381, 70.87535223163498),\n",
       " (3, 0.87): (6.1466936037706015, 70.84988487470656),\n",
       " (3, 0.88): (6.146519960360805, 70.84135786456216),\n",
       " (3, 0.89): (6.146723787608154, 70.85136720030286),\n",
       " (3, 0.9): (6.147345459258133, 70.88190433840678),\n",
       " (3, 0.91): (6.148435775016153, 70.93549353498702),\n",
       " (3, 0.92): (6.15005997784283, 71.01539850770094),\n",
       " (3, 0.93): (6.152304046928119, 71.12594679593367),\n",
       " (3, 0.94): (6.155285087290599, 71.27306626416586),\n",
       " (3, 0.95): (6.159169674758123, 71.46523406882805),\n",
       " (3, 0.96): (6.164209270523979, 71.71531163813766),\n",
       " (3, 0.97): (6.170817697877571, 72.0445652276295),\n",
       " (3, 0.98): (6.179776327795721, 72.49332858451893),\n",
       " (3, 0.99): (6.193000786159116, 73.16089333361225),\n",
       " (3, 1.0): (6.224983220132772, 74.80087390005508),\n",
       " (4, 0.6): (6.32173318385284, 79.98919256734635),\n",
       " (4, 0.61): (6.310883838155776, 79.38991453845604),\n",
       " (4, 0.62): (6.300331167696529, 78.81133135781494),\n",
       " (4, 0.63): (6.290070514835182, 78.25280245350473),\n",
       " (4, 0.64): (6.280097756314168, 77.71373826798248),\n",
       " (4, 0.65): (6.270409299686299, 77.19359806974447),\n",
       " (4, 0.66): (6.261002083700958, 76.6918881326145),\n",
       " (4, 0.67): (6.251873582862484, 76.20816027472455),\n",
       " (4, 0.68): (6.243021816483492, 75.742010756768),\n",
       " (4, 0.69): (6.234445362636025, 75.29307954475561),\n",
       " (4, 0.7): (6.2261433775387, 74.86104995080143),\n",
       " (4, 0.71): (6.21811562106844, 74.44564867425858),\n",
       " (4, 0.72): (6.210362489263971, 74.04664627552815),\n",
       " (4, 0.73): (6.2028850549182355, 73.66385812729312),\n",
       " (4, 0.74): (6.1956851176482415, 73.29714590328092),\n",
       " (4, 0.75): (6.188765265200006, 72.94641968376409),\n",
       " (4, 0.76): (6.182128948217046, 72.61164078108169),\n",
       " (4, 0.77): (6.175780571336963, 72.29282542072276),\n",
       " (4, 0.78): (6.169725604279294, 71.99004945379843),\n",
       " (4, 0.79): (6.163970717717259, 71.70345433354063),\n",
       " (4, 0.8): (6.158523950183522, 71.43325466164534),\n",
       " (4, 0.81): (6.153394914329644, 71.17974771414453),\n",
       " (4, 0.82): (6.148595053677368, 70.94332549797535),\n",
       " (4, 0.83): (6.14413796505474, 70.72449009307438),\n",
       " (4, 0.84): (6.140039807684201, 70.5238733251602),\n",
       " (4, 0.85): (6.136319828424412, 70.34226224397594),\n",
       " (4, 0.86): (6.133001045436476, 70.1806325253948),\n",
       " (4, 0.87): (6.130111152190458, 70.04019290739232),\n",
       " (4, 0.88): (6.127683734747305, 69.9224453378489),\n",
       " (4, 0.89): (6.125759945782385, 69.82926807008792),\n",
       " (4, 0.9): (6.124390864109637, 69.7630332658996),\n",
       " (4, 0.91): (6.1236409185234555, 69.72677828367242),\n",
       " (4, 0.92): (6.123593031944027, 69.72446391965357),\n",
       " (4, 0.93): (6.1243566846136135, 69.76138050006547),\n",
       " (4, 0.94): (6.126081236659267, 69.84482091010914),\n",
       " (4, 0.95): (6.128979476505956, 69.9852736752864),\n",
       " (4, 0.96): (6.133373150360531, 70.19873609286003),\n",
       " (4, 0.97): (6.13979281254962, 70.51180039077775),\n",
       " (4, 0.98): (6.149243429169796, 70.97521598767128),\n",
       " (4, 0.99): (6.164206252002217, 71.71516158979327),\n",
       " (4, 1.0): (6.204572935490571, 73.75009156760723),\n",
       " (5, 0.6): (6.397004795799044, 84.2733631523835),\n",
       " (5, 0.61): (6.383162661402387, 83.46865747463698),\n",
       " (5, 0.62): (6.369672415324392, 82.69180266483126),\n",
       " (5, 0.63): (6.356528028335745, 81.94181981521773),\n",
       " (5, 0.64): (6.343724099233174, 81.21780314042937),\n",
       " (5, 0.65): (6.331255848223993, 80.51891619428775),\n",
       " (5, 0.66): (6.319119114837498, 79.84438861154909),\n",
       " (5, 0.67): (6.3073103605905265, 79.19351335317596),\n",
       " (5, 0.68): (6.2958266767289714, 78.5656444420083),\n",
       " (5, 0.69): (6.28466579747713, 77.96019518428636),\n",
       " (5, 0.7): (6.273826119370451, 77.37663688236034),\n",
       " (5, 0.71): (6.263306727409037, 76.81449805425974),\n",
       " (5, 0.72): (6.253107428968093, 76.27336418765674),\n",
       " (5, 0.73): (6.243228796655109, 75.75287807007608),\n",
       " (5, 0.74): (6.233672221621257, 75.25274075443927),\n",
       " (5, 0.75): (6.224439979241216, 74.77271324046713),\n",
       " (5, 0.76): (6.215535309587813, 74.31261897890951),\n",
       " (5, 0.77): (6.206962515824134, 73.87234734106048),\n",
       " (5, 0.78): (6.19872708451502, 73.45185824036398),\n",
       " (5, 0.79): (6.190835833089446, 73.05118815467013),\n",
       " (5, 0.8): (6.1832970912843255, 72.67045787764962),\n",
       " (5, 0.81): (6.1761209256533345, 72.30988244045196),\n",
       " (5, 0.82): (6.169319419335512, 71.9697838000277),\n",
       " (5, 0.83): (6.162907023679932, 71.65060711040785),\n",
       " (5, 0.84): (6.156901004669185, 71.35294171056923),\n",
       " (5, 0.85): (6.151322016417816, 71.07754842980866),\n",
       " (5, 0.86): (6.146194847997669, 70.8253955123436),\n",
       " (5, 0.87): (6.141549411357495, 70.59770654350325),\n",
       " (5, 0.88): (6.1374220720839805, 70.39602546951845),\n",
       " (5, 0.89): (6.1338574800855055, 70.22230659196687),\n",
       " (5, 0.9): (6.13091115076203, 70.07904213811567),\n",
       " (5, 0.91): (6.128653211657931, 69.96944832598689),\n",
       " (5, 0.92): (6.127174033468972, 69.89774624080367),\n",
       " (5, 0.93): (6.126593059451137, 69.86960405009052),\n",
       " (5, 0.94): (6.127073399593519, 69.89287075737302),\n",
       " (5, 0.95): (6.128847637987709, 69.97887846843726),\n",
       " (5, 0.96): (6.132267756099228, 70.14497035729791),\n",
       " (5, 0.97): (6.137914693174145, 70.42006692467974),\n",
       " (5, 0.98): (6.146890068216451, 70.85953378233292),\n",
       " (5, 0.99): (6.1619186844579295, 71.60153862929413),\n",
       " (5, 1.0): (6.205213612170486, 73.78285002000437)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Q4) lowest cross-entropy/perplexity model ((4, 0.92), (6.123593031944027, 69.72446391965357))\n",
      "(Q4) lowest cross-entropy/perplexity 3-gram model ((3, 0.88), (6.146519960360805, 70.84135786456216))\n",
      "(Q4) lowest cross-entropy/perplexity 4-gram model ((4, 0.92), (6.123593031944027, 69.72446391965357))\n",
      "(Q4) lowest cross-entropy/perplexity 5-gram model ((5, 0.93), (6.126593059451137, 69.86960405009052))\n"
     ]
    }
   ],
   "source": [
    "# find the best discounts for each n-gram and overall\n",
    "print(\"(Q4) lowest cross-entropy/perplexity model\", min(results.items(), key=lambda x:x[1][1]))\n",
    "for n in [3, 4, 5]:\n",
    "    ngram = filter(lambda x:x[0][0]==n, results.items())\n",
    "    print(\"(Q4) lowest cross-entropy/perplexity {}-gram model\".format(n), min(ngram, key=lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try optimization for each order and get the best value\n",
    "\n",
    "def perplexity_kn_ngram(discount, order, sentences):\n",
    "    print(discount, order)\n",
    "    \n",
    "    #print(\"*\" * 30)\n",
    "    #print(\"order\", order)\n",
    "    print(\"discount\", discount)\n",
    "\n",
    "    N = 0\n",
    "    s = 0\n",
    "    unknown_words = 0\n",
    "    for sent in sentences:\n",
    "        tokens = tokenize_sentence(sent, order)\n",
    "        for i in range(order-1, len(tokens)):\n",
    "            N +=1\n",
    "            ngram = tokens[i-delta:i+1]\n",
    "            kn_prob = kneser_ney_ngram_prob(ngram, discount, order)\n",
    "            s+= (-log(kn_prob, 2))\n",
    "    perplexity = 2 ** (s/N)\n",
    "    print(order, discount, s/N, perplexity)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8] 3\n",
      "discount [0.8]\n",
      "3 [0.8] 6.156343411773763 71.32536955933497\n",
      "[0.84] 3\n",
      "discount [0.84]\n",
      "3 [0.84] 6.149181348370975 70.97216190958851\n",
      "[0.88] 3\n",
      "discount [0.88]\n",
      "3 [0.88] 6.146519960360803 70.84135786456207\n",
      "[0.92] 3\n",
      "discount [0.92]\n",
      "3 [0.92] 6.15005997784283 71.01539850770094\n",
      "[0.92] 3\n",
      "discount [0.92]\n",
      "3 [0.92] 6.15005997784283 71.01539850770094\n",
      "[0.86] 3\n",
      "discount [0.86]\n",
      "3 [0.86] 6.147212094770381 70.87535223163498\n",
      "[0.9] 3\n",
      "discount [0.9]\n",
      "3 [0.9] 6.147345459258133 70.88190433840678\n",
      "[0.87] 3\n",
      "discount [0.87]\n",
      "3 [0.87] 6.146693603770601 70.84988487470652\n",
      "[0.89] 3\n",
      "discount [0.89]\n",
      "3 [0.89] 6.146723787608154 70.85136720030286\n",
      "[0.875] 3\n",
      "discount [0.875]\n",
      "3 [0.875] 6.146561794042411 70.84341207400546\n",
      "[0.885] 3\n",
      "discount [0.885]\n",
      "3 [0.885] 6.146572374800697 70.84393164310829\n",
      "[0.8775] 3\n",
      "discount [0.8775]\n",
      "3 [0.8775] 6.146529373623677 70.84182009010561\n",
      "[0.8825] 3\n",
      "discount [0.8825]\n",
      "3 [0.8825] 6.146534101681184 70.84205225611345\n",
      "[0.87875] 3\n",
      "discount [0.87875]\n",
      "3 [0.87875] 6.14652175755727 70.84144611323124\n",
      "[0.88125] 3\n",
      "discount [0.88125]\n",
      "3 [0.88125] 6.146524051344808 70.8415587464287\n",
      "[0.879375] 3\n",
      "discount [0.879375]\n",
      "3 [0.879375] 6.146520127311352 70.84136606241675\n",
      "[0.880625] 3\n",
      "discount [0.880625]\n",
      "3 [0.880625] 6.146521265426291 70.84142194785787\n",
      "[0.8796875] 3\n",
      "discount [0.8796875]\n",
      "3 [0.8796875] 6.146519860381825 70.84135295524574\n",
      "[0.879375] 3\n",
      "discount [0.879375]\n",
      "3 [0.879375] 6.146520127311352 70.84136606241675\n",
      "[0.87984375] 3\n",
      "discount [0.87984375]\n",
      "3 [0.87984375] 6.146519864439866 70.84135315450969\n",
      "[0.87953125] 3\n",
      "discount [0.87953125]\n",
      "3 [0.87953125] 6.146519948051098 70.8413572601126\n",
      "[0.87976563] 3\n",
      "discount [0.87976563]\n",
      "3 [0.87976563] 6.146519850936172 70.84135249143127\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 70.841352\n",
      "         Iterations: 11\n",
      "         Function evaluations: 22\n",
      "best k for n= 3 : 0.8797656250000001\n",
      "[0.8] 4\n",
      "discount [0.8]\n",
      "4 [0.8] 6.158523950183522 71.43325466164534\n",
      "[0.84] 4\n",
      "discount [0.84]\n",
      "4 [0.84] 6.140039807684201 70.5238733251602\n",
      "[0.88] 4\n",
      "discount [0.88]\n",
      "4 [0.88] 6.127683734747305 69.9224453378489\n",
      "[0.92] 4\n",
      "discount [0.92]\n",
      "4 [0.92] 6.1235930319440275 69.72446391965362\n",
      "[1.] 4\n",
      "discount [1.]\n",
      "4 [1.] 6.204572935490601 73.75009156760878\n",
      "[0.88] 4\n",
      "discount [0.88]\n",
      "4 [0.88] 6.127683734747305 69.9224453378489\n",
      "[0.96] 4\n",
      "discount [0.96]\n",
      "4 [0.96] 6.133373150360532 70.19873609286007\n",
      "[0.9] 4\n",
      "discount [0.9]\n",
      "4 [0.9] 6.124390864109637 69.7630332658996\n",
      "[0.94] 4\n",
      "discount [0.94]\n",
      "4 [0.94] 6.126081236659267 69.84482091010914\n",
      "[0.91] 4\n",
      "discount [0.91]\n",
      "4 [0.91] 6.1236409185234555 69.72677828367242\n",
      "[0.93] 4\n",
      "discount [0.93]\n",
      "4 [0.93] 6.1243566846136135 69.76138050006547\n",
      "[0.915] 4\n",
      "discount [0.915]\n",
      "4 [0.915] 6.123523120124431 69.7210852111268\n",
      "[0.91] 4\n",
      "discount [0.91]\n",
      "4 [0.91] 6.1236409185234555 69.72677828367242\n",
      "[0.9175] 4\n",
      "discount [0.9175]\n",
      "4 [0.9175] 6.123533752488727 69.7215990430093\n",
      "[0.9125] 4\n",
      "discount [0.9125]\n",
      "4 [0.9125] 6.12355938474868 69.72283779269394\n",
      "[0.91625] 4\n",
      "discount [0.91625]\n",
      "4 [0.91625] 6.123522467811075 69.72105368680356\n",
      "[0.9175] 4\n",
      "discount [0.9175]\n",
      "4 [0.9175] 6.123533752488727 69.7215990430093\n",
      "[0.915625] 4\n",
      "discount [0.915625]\n",
      "4 [0.915625] 6.123521315442915 69.7209979963853\n",
      "[0.915] 4\n",
      "discount [0.915]\n",
      "4 [0.915] 6.123523120124431 69.7210852111268\n",
      "[0.9159375] 4\n",
      "discount [0.9159375]\n",
      "4 [0.9159375] 6.123521520315281 69.72100789723505\n",
      "[0.9153125] 4\n",
      "discount [0.9153125]\n",
      "4 [0.9153125] 6.123521849825223 69.7210238214375\n",
      "[0.91578125] 4\n",
      "discount [0.91578125]\n",
      "4 [0.91578125] 6.123521325262367 69.72099847092909\n",
      "[0.91546875] 4\n",
      "discount [0.91546875]\n",
      "4 [0.91546875] 6.123521490436335 69.72100645327765\n",
      "[0.91570313] 4\n",
      "discount [0.91570313]\n",
      "4 [0.91570313] 6.123521297225009 69.72099711597016\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 69.720997\n",
      "         Iterations: 12\n",
      "         Function evaluations: 24\n",
      "best k for n= 4 : 0.9157031250000004\n",
      "[0.8] 5\n",
      "discount [0.8]\n",
      "5 [0.8] 6.1832970912843255 72.67045787764962\n",
      "[0.84] 5\n",
      "discount [0.84]\n",
      "5 [0.84] 6.156901004669185 71.35294171056923\n",
      "[0.88] 5\n",
      "discount [0.88]\n",
      "5 [0.88] 6.1374220720839805 70.39602546951845\n",
      "[0.92] 5\n",
      "discount [0.92]\n",
      "5 [0.92] 6.127174033468972 69.89774624080367\n",
      "[1.] 5\n",
      "discount [1.]\n",
      "5 [1.] 6.205213612170524 73.7828500200063\n",
      "[0.88] 5\n",
      "discount [0.88]\n",
      "5 [0.88] 6.1374220720839805 70.39602546951845\n",
      "[0.96] 5\n",
      "discount [0.96]\n",
      "5 [0.96] 6.132267756099231 70.14497035729805\n",
      "[0.94] 5\n",
      "discount [0.94]\n",
      "5 [0.94] 6.127073399593518 69.89287075737293\n",
      "[0.96] 5\n",
      "discount [0.96]\n",
      "5 [0.96] 6.132267756099229 70.14497035729795\n",
      "[0.93] 5\n",
      "discount [0.93]\n",
      "5 [0.93] 6.126593059451137 69.86960405009052\n",
      "[0.92] 5\n",
      "discount [0.92]\n",
      "5 [0.92] 6.127174033468972 69.89774624080367\n",
      "[0.935] 5\n",
      "discount [0.935]\n",
      "5 [0.935] 6.126688075616534 69.8742058268248\n",
      "[0.925] 5\n",
      "discount [0.925]\n",
      "5 [0.925] 6.126762328082607 69.877802197138\n",
      "[0.9325] 5\n",
      "discount [0.9325]\n",
      "5 [0.9325] 6.126606015516022 69.87023151408161\n",
      "[0.9275] 5\n",
      "discount [0.9275]\n",
      "5 [0.9275] 6.126646109972501 69.87217332978423\n",
      "[0.93125] 5\n",
      "discount [0.93125]\n",
      "5 [0.93125] 6.126591099498505 69.86950912980767\n",
      "[0.9325] 5\n",
      "discount [0.9325]\n",
      "5 [0.9325] 6.126606015516022 69.87023151408161\n",
      "[0.930625] 5\n",
      "discount [0.930625]\n",
      "5 [0.930625] 6.126589994015022 69.86945559142534\n",
      "[0.93] 5\n",
      "discount [0.93]\n",
      "5 [0.93] 6.126593059451137 69.86960405009052\n",
      "[0.9309375] 5\n",
      "discount [0.9309375]\n",
      "5 [0.9309375] 6.12659002242848 69.86945696748387\n",
      "[0.9303125] 5\n",
      "discount [0.9303125]\n"
     ]
    }
   ],
   "source": [
    "overall_best_perplex = 10000\n",
    "best_order_k = (0,0)\n",
    "for order in range(3,6):\n",
    "    # reset each time\n",
    "    delta = order -1\n",
    "    unigram_denominator = 0\n",
    "    ngram_numerator_map = Counter() \n",
    "    ngram_denominator_map = Counter() \n",
    "    ngram_non_zero_map = Counter()\n",
    "\n",
    "    # for each n- get the counts from training\n",
    "    for sent in train_sentences:\n",
    "        tokens = tokenize_sentence(sent, order)\n",
    "        # correct use of kneser-ney probability function:\n",
    "        ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "                count_ngrams_interpolated_kneser_ney(tokens, order,\n",
    "                                               ngram_numerator_map, ngram_denominator_map,\n",
    "                                      ngram_non_zero_map, unigram_denominator)\n",
    "\n",
    "    best = optimize.minimize(\n",
    "        perplexity_kn_ngram,\n",
    "        0.8,    # first argument to be optimized\n",
    "        args=(order, heldout_sentences),  # other arguments to the function\n",
    "        method='Nelder-Mead', # use nelder mead to find minima\n",
    "        tol=0.0001, # to this degree of error\n",
    "        options={'disp': True}\n",
    "    )\n",
    "    k = best.x[0]\n",
    "    if best.fun < overall_best_perplex:\n",
    "        overall_best_perplex = best.fun\n",
    "        best_order_k = (order, k)\n",
    "    print(\"best k for n=\",order,\":\", k)\n",
    "print(\"Overall order, k\", best_order_k, \"perplexity:\", overall_best_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_order_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best order and discount on test data\n",
    "order = best_order_k[0]\n",
    "discount = best_order_k[1]\n",
    "    \n",
    "delta = order -1\n",
    "unigram_denominator = 0\n",
    "ngram_numerator_map = Counter() \n",
    "ngram_denominator_map = Counter() \n",
    "ngram_non_zero_map = Counter()\n",
    "\n",
    "# for each n- get the counts from training\n",
    "for sent in train_sentences:\n",
    "    tokens = tokenize_sentence(sent, order)\n",
    "    # correct use of kneser-ney probability function:\n",
    "    ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "            count_ngrams_interpolated_kneser_ney(tokens, order,\n",
    "                                           ngram_numerator_map, ngram_denominator_map,\n",
    "                                  ngram_non_zero_map, unigram_denominator)\n",
    "\n",
    "print(\"perplexity on test\", perplexity_kn_ngram(discount, order, test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
